{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfUAqOV92ot"
      },
      "source": [
        "# Week 6. Optimization. Programming Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkCJL1Wf92oy"
      },
      "source": [
        "Let us consider the **House Pricing** dataset, where you have a lot of information about the houses being sold and you aim to produce the price of the house.\n",
        "\n",
        "\n",
        "Firstly, let us import basic libraries (`numpy` ([docs](https://numpy.org/)) for matrix operations and `pandas` ([docs](https://pandas.pydata.org/)) for convinient dataset workaround):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrGBxXz092oy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1csIuXge92oz"
      },
      "source": [
        "### Task 1. Reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDpmN22q92oz"
      },
      "outputs": [],
      "source": [
        "datX=np.load('x_train.npy')\n",
        "datY=np.log(np.load('y_train.npy'))\n",
        "datX=pd.DataFrame(datX, columns=datX.dtype.names)\n",
        "datX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVsuJADv92o0"
      },
      "source": [
        "Okay, we manage to load the data (you can read more about the `load` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html). But it is not a necessity). We are going to use linear models to work with it, but firstly we need to come up with idea what features should we include in the model at all (which feature the price is lineary dependent on):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBQWJ2XM92o0"
      },
      "source": [
        "Do not forget to install seaborn. You can do that by running `pip install seaborn` in the command line locally, or simply by running the next sell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQJ_jhYg92o0"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2psfPlin92o1"
      },
      "source": [
        "In order to do it let us plot every feature vs the price. Firstly, we import nice plotting modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MQDjQee92o1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1cC0KjD92o1"
      },
      "outputs": [],
      "source": [
        "f, ax=plt.subplots(4, 4, figsize=(16,16))\n",
        "\n",
        "for i, name in enumerate(datX.columns):\n",
        "    ax[i//4][i%4].scatter(datX[name], datY)\n",
        "    ax[i//4][i%4].set_title(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHwcf1eM92o1"
      },
      "source": [
        "Let us say, that we choose to work the following set of features:\n",
        "+ `bedrooms`\n",
        "+ `bathrooms`\n",
        "+ `sqft_living`\n",
        "+ `floors`\n",
        "+ `condition`\n",
        "+ `grade`\n",
        "+ `sqft_above`\n",
        "+ `sqft_basement`\n",
        "+ `long`\n",
        "+ `lat`\n",
        "\n",
        "Clear the dataset from all the other features and create:\n",
        "1. matrix $X$, all elements should be real numbers\n",
        "2. number $N$ -- number of considered houses\n",
        "3. number $m$ -- number of features\n",
        "\n",
        "**Hint**: it is easier to clean columns from dataset (you should look [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) for insipration) and the get a matrix with `.values`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR7vn7Rs92o2"
      },
      "outputs": [],
      "source": [
        "#your code goes here\n",
        "X=...\n",
        "N=...\n",
        "m=..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhqC9m_D92o2"
      },
      "source": [
        "Consider that we are interested in the loss of the model we discussed in the video:\n",
        "\n",
        "+ Assume we have input data that is denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_N$\n",
        "+ House prices for this input data are known $y_1, y_2, \\ldots, y_N$\n",
        "\n",
        "We propose a **simple linear model** for this task:\n",
        "\n",
        "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$\n",
        "\n",
        "As a loss function we will use the mean squared error (**MSE**):\n",
        "\n",
        "$$\n",
        "Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "### Task 2. Compute analytically the $Loss(\\vec{w})$  function  (1 point)\n",
        "Please, keep the signature of the function and enter the code only under `your code goes here`.\n",
        "\n",
        "**Attention**: you need to avoid usage of `for` cycles! The easiest way to do it is by using matrix operations.\n",
        "\n",
        "_Hint_: to get nice $w_0$ coefficient it is convinient to add to the `X` matrix the column of 1 with `np.concatenate` [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt8KhWCH92o3"
      },
      "outputs": [],
      "source": [
        "def loss(w, X, y):\n",
        "    #your code goes here\n",
        "\n",
        "    return lossValue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhZHpmdb92o3"
      },
      "source": [
        "### Task 3. Compute analyticaly the gradient of the $Loss(\\vec{w})$ (1 point)\n",
        "Please, enter your answer in the cell below (it should be a `markdown` cell). You can initially specify each partial derivative $\\frac{\\partial Loss}{\\partial w_i}$, but **your final answer must consists of $\\nabla Loss$ altogether using matrix operations**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBNxjGyn92o3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfwA2dgK92o3"
      },
      "source": [
        "### Task 4. Write a function to compute the gradient of the Loss function in the given point  (1 point)\n",
        "Please, keep the signature of the function and enter the code only under `your code goes here`.\n",
        "\n",
        "**Attention**: you need to avoid usage of `for` cycles! The easiest way to do it is by using matrix operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvwwFw9192o3"
      },
      "outputs": [],
      "source": [
        "def grad(w_k, X, y):\n",
        "    #your code goes here\n",
        "\n",
        "    return lossGradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONR6HgXs92o4"
      },
      "source": [
        "### Task 5. Write gradient descent (2 points)\n",
        "How it is time to formulate the gradient descent! As you remeember, the idea here is that:\n",
        "$$\n",
        "\\vec{w}^{k+1}=\\vec{w}^{k}-\\alpha_k\\cdot \\nabla Loss(\\vec{w}^{k})\n",
        "$$\n",
        "We propose that you use constant $\\alpha_k=\\alpha$. Assume that the method should stop in two cases:\n",
        "+ if the number of iterations is to high (`maxiter`)\n",
        "+ if the length of the gradient is low enough (<`eps`) to call an extremum\n",
        "\n",
        "Please, keep the signature of the function and enter the code only under `your code goes here`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smAnSyrn92o4"
      },
      "outputs": [],
      "source": [
        "def gradDescent(w_init, alpha, X, y, maxiter=500, eps=1e-2):\n",
        "    losses=[]\n",
        "    weights=[w_init]\n",
        "\n",
        "    curiter=0\n",
        "    w_k=weights[-1]\n",
        "\n",
        "    #your code goes here\n",
        "    while ...:\n",
        "        w_k=...\n",
        "        lossValue_k=...\n",
        "\n",
        "        weights.append(w_k)\n",
        "        losses.append(lossValue_k)\n",
        "\n",
        "    return weights, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu7p6rh092o4"
      },
      "source": [
        "Experiment with several alphas and several intial values of weights. To illustrate, provide graphs for the Loss function over iterations in each case (and, optionally, the distance between weigths from one iteration to the next):\n",
        "\n",
        "(we provided all key plotting commands for you, but you can always look into [this tutorial](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py))\n",
        "\n",
        "**Note:** You need to provide at least **two** experiments with **different values of $\\alpha$** (**1 point**). Preferably, there should be at least one convergent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkHYOx1792o4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "#your code goes here\n",
        "\n",
        "plt.plot(...)\n",
        "plt.xlabel(...)\n",
        "plt.ylabel(...)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ty9QwZ92o4"
      },
      "source": [
        "Let us check the adequacy of the model we created.\n",
        "\n",
        "Choose several (no less then five) houses (inputs in your `X` matrix) and calculte predicted prices by:\n",
        "\n",
        "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPMPMXjy92o4"
      },
      "outputs": [],
      "source": [
        "### your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_nCTM5E92o4"
      },
      "source": [
        "Compare predicted values with an actual answer (stored in your `y` array). Is it satisfying enough? (**1 point**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf-Jfp4s92o5"
      },
      "source": [
        "### Task 6. Data transformation\n",
        "\n",
        "As you have probably already seen above, the convergence of the gradient descent is not ideal for our data. One way to overcome this is to transform the input data so that:\n",
        "+ the **average** of each feature should be $0$\n",
        "+ the **standard deviation** of each feature should be $1$\n",
        "\n",
        "In such a way levels of the loss function would be close to circles; thus one should hope to faster convergence.\n",
        "\n",
        "Implement such normalisation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAb1ozP492o5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3ll6QHe92o5"
      },
      "outputs": [],
      "source": [
        "def norm(X):\n",
        "    # your code goes here\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_eMWjY-92o5"
      },
      "source": [
        "### Task 7. And again (2 points)\n",
        "\n",
        "Repeat gradient descent experiments for different $\\alpha$s, now with transformed data.\n",
        "\n",
        "+ Run at least two experiments with different $\\alpha$\n",
        "+ At least two experiments should be convergent\n",
        "\n",
        "1. Provide `loss` plots for those experiments (on the same graph). (**1 point**)\n",
        "2. Are optimized weights the same? Illustrate it (plot it or show differences in other way)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCZwDW1g92o5"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEf-jdtF92o5"
      },
      "source": [
        "### Task 8. Better pay twice  (1 point)\n",
        "\n",
        "Sometimes it is essential to alter the loss function and make it assymetric. Normally, it is motivated by the task itself. For instance, in our case assume that one uses our prediction to bid for an apartment: hence if our $\\hat{y}>y$ then we will overpay, but if $\\hat{y}<y$ we will not get an apartment, but also won't lose any money.\n",
        "\n",
        "Let us introduce our new function:\n",
        "$$\n",
        "Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N \\begin{cases} a(y_i-\\hat{y}_i)^2, \\quad y_i>\\hat{y}_i \\\\ b(y_i-\\hat{y}_i)^2, \\quad y_i\\le\\hat{y}_i \\end{cases}\n",
        "$$\n",
        "\n",
        "Implement new loss and new gradient:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjPQXMpx92o5"
      },
      "outputs": [],
      "source": [
        "def new_loss(w, X, y, a, b):\n",
        "    #your code goes here\n",
        "\n",
        "    return lossValue\n",
        "\n",
        "def new_grad(w_k, X, y, a, b):\n",
        "    #your code goes here\n",
        "\n",
        "    return lossGradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2AWyoyt92o6"
      },
      "source": [
        "### Task 9. More experiments! (2 points)\n",
        "\n",
        "Now let us experiment with new functions (**2 points**)\n",
        "\n",
        "1. Assume your data was normalised (otherwise repeat **Task 6**)\n",
        "2. Select at least two pairs of $(a,b)$ parameters such that $a_1/b_1>1$ and $a_2/b_2<1$\n",
        "3. Run **gradient descent** with new function and given parameters\n",
        "4. Make a `loss` plot for each expriment (please, provide legend!)\n",
        "5. Check whether you've got coinciding weights and _illustrate_ it\n",
        "\n",
        "You may also alter $\\alpha$ and provide more experiments on composite relation between $a/b$ and $\\alpha$ values (_optional_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh4lrUmH92o6"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3qh-sk292o6"
      },
      "source": [
        "### Task 10. Discussion (1 point)\n",
        "Answer following questions:\n",
        "1. Have you managed to get sufficiently different weights with different $\\alpha$ or $(a,b)$ parameters of assymetry? What does it mean?\n",
        "2. Assume $a$ and $b$ are not given by the task and you need to choose them with the data. Propose a strategy of doing that (assume $a=1$ and choosing only $b$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OblRo-gE92o6"
      },
      "source": [
        "1.\n",
        "\n",
        "2.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}